from Compiler import ml
from Compiler import lr

import math
import re

from Compiler import mpc_math, util
from Compiler.types import *
from Compiler.types import _unreduced_squant
from Compiler.library import *
from Compiler.util import is_zero, tree_reduce
from Compiler.comparison import CarryOutRawLE
from Compiler.GC.types import sbitint
from functools import reduce
import math

# NOTE: This script should not be run alone. The entry point to run the program is in run.py.
#       run.py must be run first for the sake of some book-keeping; for example, run.py will
#       have Alice and Bob populate their private input files which allows for us to combine
#       their horizontally partitioned data into one dataset.


ALICE = 0
BOB = 1

n_threads = 8

# It appears that batching is maxed out at 128
max_batch = 128

# No traditional command line arguments? Solution is to re-write the script
# using another python script which can also act as the entry point
# @args
alice_examples = 831
bob_examples = 882
n_features = 1875
n_epochs = 13
folds = 5
# end@args

n_examples = alice_examples + bob_examples

train_size_alice = alice_examples

train_size_bob = bob_examples

# print_ln("%s", train_size)

data_train = Matrix(n_examples, n_features, sfix)

label_train = Array(n_examples, sfix)

print_ln("\ttrain size: %s", n_examples)
print_ln("\ttrain_size_alice: %s ", train_size_alice)
print_ln("\ttrain_size_bob: %s ", train_size_bob)


# Reference: self.X = MultiArray([N, d, d_in], sfix)

# First: Training features
@for_range(train_size_alice)
def _(i):
    @for_range(n_features)
    def _(j):
        data_train[i][j] = sfix.get_input_from(ALICE)

@for_range(train_size_alice)
def _(i):
    label_train[i] = sfix.get_input_from(ALICE)


@for_range(train_size_bob)
def _(i):
    @for_range(n_features)
    def _(j):
        data_train[train_size_alice + i][j] = sfix.get_input_from(BOB)

@for_range(train_size_bob)
def _(i):
    label_train[train_size_alice + i] = sfix.get_input_from(BOB)


print_ln("%s", "\n\n Data populated \n\n")

lr = lr.LogisticRegression(data_train, label_train, iterations=n_epochs, learning_rate=0.001)

w, b = lr.train()

# For classification
print_ln("Training finished")
print_ln("%s", n_epochs)
# print_ln("%s", sgd.layers[0].N)
print_ln("%s", b.reveal())
print_ln("%s", w.reveal_nested())
