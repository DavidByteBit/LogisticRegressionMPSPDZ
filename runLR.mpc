from Compiler import ml
from Compiler.types import _secret

# NOTE: This script should not be run alone. The entry point to run the program is in run.py.
#       run.py must be run first for the sake of some book-keeping; for example, run.py will
#       have Alice and Bob populate their private input files which allows for us to combine
#       their horizontally partitioned data into one dataset.

ALICE = 0
BOB = 1

# PURELY FOR TESTING PURPOSES - reduces the amount of data going in by a factor of n to test compilers
# ability to work with different loads
reduce_set = 2  # Anything lower than 49 does not work...

# It appears that batching is maxed out at 128
max_batch = 128

# No traditional command line arguments? Solution is to re-write the script
# using another python script which can also act as the entry point
# @args
alice_examples = 831
bob_examples = 882
n_features = 1875
n_epochs = 13
folds = 5
# end@args

n_examples = alice_examples + bob_examples

data = Matrix(n_examples, n_features, sfix)

test_ratio = 1 / folds

# TODO: train_size should use ceil
train_size = int(n_examples * (1 - test_ratio))
test_size = int(n_examples * test_ratio)

# Load data from Alice first, then Bob

# Reference: self.X = MultiArray([N, d, d_in], sfix)
@for_range_opt(alice_examples)
def _(i):
    @for_range_opt(n_features)
    def _(j):
        data[i][j] = sfix.get_input_from(ALICE)
        # sgd.layers[0].X.input_from(ALICE)


print_ln("%s", "\n\n Alice's data is loaded \n\n")


@for_range_opt(bob_examples)
def _(i):
    @for_range_opt(n_features)
    def _(j):
        data[alice_examples + i][j] = sfix.get_input_from(BOB)
        # print_ln("i: %s, j: %s", i, j)
        # sgd.layers[0].X.input_from(BOB)


print_ln("%s", "\n\n Bobs data is loaded \n\n")

labels = Array(n_examples, sfix)


# Load labels from alice first to ensure that the labels remain parallel to the dataset
@for_range_opt(alice_examples)
def _(i):
    labels[i] = sfix.get_input_from(ALICE)
    # sgd.layers[1].Y.input_from(ALICE)


print_ln("%s", "\n\n Alice's labels is loaded \n\n")


@for_range_opt(bob_examples)
def _(i):
    labels[alice_examples + i] = sfix.get_input_from(BOB)
    # print_ln("i: %s", i)
    # sgd.layers[1].Y.input_from(BOB)


print_ln("%s", "\n\n Bobs labels is loaded \n\n")

@for_range(folds)
def _(f):

    train_data = Matrix(train_size, n_features, sfix)
    train_label = Array(train_size, sfix)

    test_data = Matrix(test_size, n_features, sfix)
    test_label = Array(test_size, sfix)

    # This does not change the folds at all, which is clearly not ideal; however, we can't use if statements in
    # the runtime-ready code, making this whole process somewhat difficult. This is in its current state just for
    # testing, will have to fix later TODO: actually partition dataset for corssfold validation
    @for_range_opt(test_size)
    def _(i):
        test_data[i] = data[i]
        test_label[i] = labels[i]

    @for_range_opt(train_size)
    def _(i):
        train_data[i] = data[i + test_size]
        train_label[i] = labels[i + test_size]

    # Define logistic regression model
    sgd = ml.SGD([ml.Dense(max_batch, n_features, 1),
                  ml.Output(n_examples, approx=True)], n_epochs,
                 report_loss=True)

    sgd.reset()  # <--- Can cause register overflow?

    print_ln("%s for fold %s", "\n\n model initialized \n\n", f)

    data_left_to_train = train_size

    l_lt_r = data_left_to_train < max_batch
    # Although these are all public values, the compiler complained about not being able to determine the truth
    # value of the expression "data_left_to_train < max_batch". TODO: Make this cleaner for clear values
    batch_size = data_left_to_train * (l_lt_r) + max_batch * (1 - l_lt_r)

    # write training params
    @for_range_opt(batch_size)
    def _(i):
        # print_ln("iteration: %s", i)
        @for_range_opt(n_features)
        def _(j):
            # print_ln("iteration: %s", j)
            sgd.layers[0].X[i][0][j] = train_data[i][j]
        sgd.layers[1].Y[i] = train_label[i]

    print_ln("\t performing training for batch %s", 0)

    sgd.run()

    data_left_to_train -= max_batch

    current_batch = 1

    while data_left_to_train > 0:

        l_lt_r = data_left_to_train < max_batch
        # Although these are all public values, the compiler complained about not being able to determine the truth
        # value of the expression "data_left_to_train < max_batch". TODO: Make this cleaner for clear values
        batch_size = data_left_to_train * (l_lt_r) + max_batch * (1 - l_lt_r)

        # write training params
        @for_range_opt(batch_size)
        def _(i):
            # print_ln("iteration: %s", i)
            @for_range_opt(n_features)
            def _(j):
                # print_ln("iteration: %s", j)
                sgd.layers[0].X[i][0][j] = train_data[i + current_batch * max_batch][j]
            sgd.layers[1].Y[i] = train_label[i]

        print_ln("\t performing training for batch %s", current_batch)
        print_ln("\t performing training for batch %s", current_batch)
        sgd.run()

        data_left_to_train -= max_batch
        current_batch += 1




    # @for_range_opt(max_batch)
    # def _(i):
    #     # print_ln("iteration: %s", i)
    #     @for_range_opt(n_features)
    #     def _(j):
    #         # print_ln("iteration: %s", j)
    #         sgd.layers[0].X[i][0][j] = data[i][j]
    #     sgd.layers[1].Y[i] = labels[i]
    #
    # sgd.run()
    # print_ln("%s", sgd.layers[0].W)

    weights = sgd.layers[0].W

    #print(weights)

    print_ln("%s", weights.)

    # Get test data

    # perform classificaiton

    # sgd.layers[0].X.input_from(0)
    # sgd.layers[1].Y.input_from(1)
