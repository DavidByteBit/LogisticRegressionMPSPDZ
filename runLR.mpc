from Compiler import ml
from Compiler import generate_noise
import math

# NOTE: This script should not be run alone. The entry point to run the program is in run.py.
#       run.py must be run first for the sake of some book-keeping; for example, run.py will
#       have Alice and Bob populate their private input files which allows for us to combine
#       their horizontally partitioned data into one dataset.


ALICE = 0
BOB = 1

# No traditional command line arguments? Solution is to re-write the script
# using another python script which can also act as the entry point
# @args
alice_examples = 831
bob_examples = 882
n_features = 1875
n_epochs = 13
folds = 5
lambda_ = 1
batch_size = 128
# end@args

n_examples = alice_examples + bob_examples

train_size_alice = alice_examples

train_size_bob = bob_examples

# print_ln("%s", train_size)

data_train = Matrix(n_examples, n_features, sfix)

alice_data = Matrix(alice_examples, n_features, sfix)

bob_data = Matrix(bob_examples, n_features, sfix)

label_train = Array(n_examples, sfix)

alice_label = Array(alice_examples, sfix)

bob_label = Array(bob_examples, sfix)

alice_data.input_from(ALICE)
alice_label.input_from(ALICE)

bob_data.input_from(BOB)
bob_label.input_from(BOB)


@for_range(n_examples)
def _(i):
    @for_range(n_features)
    def _(j):
        data_train[i][j] = alice_data[i][j]
        data_train[i + alice_examples][j] = bob_data[i][j]

    label_train[i] = alice_label[i]
    label_train[i + alice_examples] = bob_label[i]


print_ln("\ttrain size: %s", n_examples)
print_ln("\ttrain_size_alice: %s ", train_size_alice)
print_ln("\ttrain_size_bob: %s ", train_size_bob)

print_ln("%s", "\n\n Data populated \n\n")

# Define logistic regression model
sgd = ml.SGD([ml.Dense(n_examples, n_features, 1),
              ml.Output(n_examples, approx=3)], n_epochs,
             report_loss=True, debug=False)

sgd.reset()

sgd.layers[0].X.assign(data_train)
sgd.layers[1].Y.assign(label_train)

sgd.run(batch_size=batch_size)

print_ln("%s", n_epochs)
print_ln("%s", sgd.layers[0].N)
print_ln("%s", batch_size)

dp_noise = generate_noise.gen_noise(n_features + 1, n_examples)

bias = sgd.layers[0].b[0] + dp_noise[0]
# turns it into a list
weights_l = sgd.layers[0].W
weights_A = sfix.Array(n_features)

print_ln("%s", dp_noise.reveal())

@for_range(n_features)
def _(i):
    weights_A[i] = weights_l[0][i]

@for_range(n_features)
def _(i):
    weights_A[i] += dp_noise[i + 1]

# for i in range(n_features):
#     weights[i] += dp_noise[i + 1]

# For classification
print_ln("Training finished")
print_ln("%s", bias.reveal())
print_ln("%s", weights_A.reveal())

# # without noise
# print_ln("Training finished")
# print_ln("%s", sgd.layers[0].b[0].reveal())
# print_ln("%s", sgd.layers[0].W.reveal_nested())
