from Compiler import ml
import math

def dot_product(a, b):
    assert (len(a) == len(b))

    res = sfix(0)
    for i in range(len(a)):
        res += a[i] * b[i]

    return res

# NOTE: This script should not be run alone. The entry point to run the program is in run.py.
#       run.py must be run first for the sake of some book-keeping; for example, run.py will
#       have Alice and Bob populate their private input files which allows for us to combine
#       their horizontally partitioned data into one dataset.


ALICE = 0
BOB = 1

# It appears that batching is maxed out at 128
max_batch = 128

# No traditional command line arguments? Solution is to re-write the script
# using another python script which can also act as the entry point
# @args
alice_examples = 831
bob_examples = 882
n_features = 1875
n_epochs = 13
folds = 5
# end@args

n_examples = alice_examples + bob_examples

# print_ln("%s", folds)
# print_ln("%s", n_examples)
test_ratio = 1 / folds

# TODO: train_size should use ceil
train_size = int(math.ceil(n_examples * (1 - test_ratio)))
test_size = int(n_examples * test_ratio)

train_size_alice = int(math.ceil(alice_examples * (1 - test_ratio)))
test_size_alice = int(alice_examples * test_ratio)

train_size_bob = int(math.ceil(bob_examples * (1 - test_ratio)))
test_size_bob = int(bob_examples * test_ratio)

if folds == 1:
    train_size = n_examples
    test_size = 0

# print_ln("%s", train_size)
# print_ln("%s", test_size)

data_train = Matrix(train_size, n_features, sfix)
data_test = Matrix(test_size, n_features, sfix)

label_train = Array(train_size, sfix)
label_test = Array(test_size, sfix)

# Reference: self.X = MultiArray([N, d, d_in], sfix)

# First: training features
@for_range(train_size_alice)
def _(i):
    @for_range(n_features)
    def _(j):
        data_train[i][j] = sfix.get_input_from(ALICE)

@for_range(train_size_bob)
def _(i):
    @for_range(n_features)
    def _(j):
        data_train[train_size_alice + i][j] = sfix.get_input_from(BOB)


# Second: Training labels
@for_range(train_size_alice)
def _(i):
    label_train[i] = sfix.get_input_from(ALICE)

@for_range(train_size_bob)
def _(i):
    label_train[train_size_alice + i] = sfix.get_input_from(BOB)


# Third: testing data
@for_range(test_size_alice)
def _(i):
    @for_range(n_features)
    def _(j):
        data_test[i][j] = sfix.get_input_from(ALICE)

@for_range(test_size_bob)
def _(i):
    @for_range(n_features)
    def _(j):
        data_test[test_size_alice + i][j] = sfix.get_input_from(BOB)


# Fourth: testing labels
@for_range(test_size_alice)
def _(i):
    @for_range(n_features)
    def _(j):
        data_test[i][j] = sfix.get_input_from(ALICE)

@for_range(test_size_bob)
def _(i):
    @for_range(n_features)
    def _(j):
        data_test[test_size_alice + i][j] = sfix.get_input_from(BOB)


print("\tData storage compiled")
print_ln("%s", "\n\n Data loaded \n\n")

# TODO: Shuffle data



batch_list_order_train = Array(train_size, cint)

# Step 1: First, create a list of indices
@for_range(train_size)
def _(i):
    batch_list_order_train[i] = cint(i)


@for_range(n_epochs)
def _(e):

    # Not actually shuffling, perhaps I need to pass in a param to the compiler?
    # Step 2: Shuffle indices
    batch_list_order_train.shuffle()

    print_ln("train size: %s", train_size)
    data = Matrix(train_size, n_features, sfix)
    label = Array(train_size, sfix)

    print_ln("test size: %s", test_size)

    @for_range(train_size)
    def _(i):
        data[i] = data_train[batch_list_order_train[i]]
        label[i] = label_train[batch_list_order_train[i]]

    # Define logistic regression model
    sgd = ml.SGD([ml.Dense(max_batch, n_features, 1),
                  ml.Output(n_examples, approx=True)], 1,
                 report_loss=True)

    print_ln("max_batch: %s", max_batch)
    print_ln("features: %s", n_features)

    sgd.reset()  # <--- Can cause register overflow?

    data_left_to_train = train_size

    print_ln("data left to train %s", data_left_to_train)

    l_lt_r = data_left_to_train < max_batch
    # Although these are all public values, the compiler complained about not being able to determine the truth
    # value of the expression "data_left_to_train < max_batch". TODO: Make this cleaner for clear values
    batch_size = data_left_to_train * (l_lt_r) + max_batch * (1 - l_lt_r)

    # print_ln("batch size: %s", batch_size)
    # print_ln("feature size: %s", n_features)

    # write training params
    @for_range(batch_size)
    def _(i):
        # print_ln("iteration: %s", i)
        @for_range(n_features)
        def _(j):
            # print_ln("iteration: %s", j)
            sgd.layers[0].X[i][0][j] = data[i][j]
        sgd.layers[1].Y[i] = label[i]

    print_ln("\t performing training for batch %s", 0)

    sgd.run()

    data_left_to_train -= max_batch

    print_ln("\t data left in batch: %s", data_left_to_train)

    current_batch = 1

    while data_left_to_train > 0:

        l_lt_r = data_left_to_train < max_batch
        # Although these are all public values, the compiler complained about not being able to determine the truth
        # value of the expression "data_left_to_train < max_batch". TODO: Make this cleaner for clear values
        batch_size = data_left_to_train * (l_lt_r) + max_batch * (1 - l_lt_r)

        # write training params
        @for_range(batch_size)
        def _(i):
            # print_ln("iteration: %s", i)
            @for_range(n_features)
            def _(j):
                # print_ln("iteration: %s", j)
                sgd.layers[0].X[i][0][j] = data[i + current_batch * max_batch][j]
            sgd.layers[1].Y[i] = label[i]

        print_ln("\t performing training for batch %s", current_batch)
        print_ln("\t data left in batch: %s", data_left_to_train)
        sgd.run()

        data_left_to_train -= max_batch
        current_batch += 1




    # @for_range_opt(max_batch)
    # def _(i):
    #     # print_ln("iteration: %s", i)
    #     @for_range_opt(n_features)
    #     def _(j):
    #         # print_ln("iteration: %s", j)
    #         sgd.layers[0].X[i][0][j] = data[i][j]
    #     sgd.layers[1].Y[i] = labels[i]
    #
    # sgd.run()
    # print_ln("%s", sgd.layers[0].W)

    weights = sgd.layers[0].W
    bias = 0  # Compiler says this does not exist despite it being in the documentation... sgd.layers[1].b

    #print(weights)

    print_ln("%s", weights)
